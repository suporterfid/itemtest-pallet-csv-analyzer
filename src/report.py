# -*- coding: utf-8 -*-
from __future__ import annotations
import pandas as pd
from pathlib import Path
from typing import Callable


SHEET_RESUMO = "Resumo_por_EPC"
SHEET_UNEXPECTED = "EPCs_inesperados"
SHEET_ANTENNA = "Leituras_por_Antena"
SHEET_FLUXO = "Fluxo_ContÃ­nuo"
SHEET_EXECUTIVE = "Indicadores_Executivos"
SHEET_METADATA = "Metadata"
SHEET_POSICOES = "Posicoes_Pallet"
SHEET_STRUCTURED = "Structured_KPIs"

def write_excel(
    out_path: str,
    summary_epc: pd.DataFrame,
    unexpected: pd.DataFrame,
    ant_counts: pd.DataFrame,
    metadata: dict,
    positions_df: pd.DataFrame | None = None,
    structured_metrics: dict | None = None,
    continuous_timeline: pd.DataFrame | None = None,
    continuous_metrics: dict | None = None,
    continuous_epcs_per_minute: pd.DataFrame | pd.Series | None = None,
):
    """Persist summary artefacts to an Excel workbook.

    Parameters
    ----------
    out_path:
        Destination workbook path.
    summary_epc:
        Per-EPC aggregated statistics.
    unexpected:
        DataFrame containing the EPCs marked as unexpected.
    ant_counts:
        Aggregated antenna statistics.
    metadata:
        Dictionary with metadata parsed from the ItemTest export.
    positions_df:
        Optional pallet layout coverage table.
    structured_metrics:
        Optional dictionary with structured-mode KPIs and coverage tables.
    continuous_timeline:
        Optional timeline table generated by the continuous analysis mode.
    continuous_metrics:
        Optional dictionary with metrics and alerts for continuous mode.
    continuous_epcs_per_minute:
        Optional Series/DataFrame with aggregated counts of unique EPCs per
        minute produced by the continuous flow analysis.
    """

    out = Path(out_path)
    out.parent.mkdir(parents=True, exist_ok=True)

    metrics_info = continuous_metrics or {}
    structured_info = structured_metrics or {}
    timeline_df = None
    if continuous_timeline is not None:
        if isinstance(continuous_timeline, pd.DataFrame):
            timeline_df = continuous_timeline.copy()
        else:
            timeline_df = pd.DataFrame(continuous_timeline)
        if "entry_time" in timeline_df.columns:
            timeline_df = timeline_df.sort_values("entry_time")

    per_minute_df = None
    if continuous_epcs_per_minute is not None:
        if isinstance(continuous_epcs_per_minute, pd.Series):
            per_minute_df = (
                continuous_epcs_per_minute.sort_index()
                .rename_axis("minute")
                .reset_index()
            )
            per_minute_df.columns = ["minute", "unique_epcs"]
        else:
            per_minute_df = pd.DataFrame(continuous_epcs_per_minute)
        if not per_minute_df.empty and "minute" in per_minute_df.columns:
            per_minute_df = per_minute_df.copy()
            per_minute_df["minute"] = pd.to_datetime(
                per_minute_df["minute"], errors="coerce"
            )

    inactive_df = None
    inactive_info = metrics_info.get("inactive_periods")
    if inactive_info is not None:
        if isinstance(inactive_info, pd.DataFrame):
            inactive_df = inactive_info.copy()
        else:
            inactive_df = pd.DataFrame(inactive_info)
        if not inactive_df.empty:
            for column in ("start_time", "end_time"):
                if column in inactive_df.columns:
                    inactive_df[column] = pd.to_datetime(
                        inactive_df[column], errors="coerce"
                    )
            for numeric_column in ("duration_seconds", "gap_seconds", "gap_multiplier"):
                if numeric_column in inactive_df.columns:
                    inactive_df[numeric_column] = pd.to_numeric(
                        inactive_df[numeric_column], errors="coerce"
                    )

    concurrency_df = None
    concurrency_info = metrics_info.get("concurrency_timeline")
    if concurrency_info is not None:
        if isinstance(concurrency_info, pd.DataFrame):
            concurrency_df = concurrency_info.copy()
        else:
            concurrency_df = pd.DataFrame(concurrency_info)
        if not concurrency_df.empty and "timestamp" in concurrency_df.columns:
            concurrency_df = concurrency_df.sort_values("timestamp")
            concurrency_df["timestamp"] = pd.to_datetime(
                concurrency_df["timestamp"], errors="coerce"
            )

    executive_rows: list[dict[str, object]] = []


    def _build_alert_lines() -> list[str]:
        alerts = [str(alert) for alert in metrics_info.get("alerts", []) if alert]
        if alerts:
            return alerts
        lines: list[str] = []
        anomalous = metrics_info.get("anomalous_epcs") or []
        if anomalous:
            sample = ", ".join(anomalous[:5])
            suffix = " ..." if len(anomalous) > 5 else ""
            lines.append(
                f"EPCs with atypical dwell time ({len(anomalous)}): {sample}{suffix}"
            )
        flag_labels = {
            "epcs_only_top_antennas": "EPCs restricted to upper antennas",
            "epcs_without_antenna": "EPCs without an identified antenna",
            "invalid_data": "Invalid data encountered",
        }
        for key, values in (metrics_info.get("inconsistency_flags") or {}).items():
            if not values:
                continue
            label = flag_labels.get(key, str(key))
            sample = ", ".join(values[:5])
            suffix = " ..." if len(values) > 5 else ""
            lines.append(f"{label} ({len(values)}): {sample}{suffix}")
        return lines

    def _format_timestamp(value: object) -> str:
        if value is None:
            return "N/A"
        try:
            ts = pd.to_datetime(value)
        except Exception:
            return str(value)
        if pd.isna(ts):
            return "N/A"
        return ts.strftime("%Y-%m-%d %H:%M:%S")

    def _append_metric(
        label: str, value: object, formatter: Callable[[object], object] | None = None
    ) -> None:
        if value is None:
            if metrics_info:
                metrics_rows.append({"Metric": label, "Value": "N/A"})
            return
        if isinstance(value, float) and pd.isna(value):
            if metrics_info:
                metrics_rows.append({"Metric": label, "Value": "N/A"})
            return
        if isinstance(value, pd.Timestamp) and pd.isna(value):
            if metrics_info:
                metrics_rows.append({"Metric": label, "Value": "N/A"})
            return
        formatted = formatter(value) if formatter else value
        metrics_rows.append({"Metric": label, "Value": formatted})

    def _append_executive(
        label: str, value: object, formatter: Callable[[object], object] | None = None
    ) -> None:
        if value is None:
            return
        if isinstance(value, float) and pd.isna(value):
            return
        if isinstance(value, pd.Timestamp) and pd.isna(value):
            return
        formatted = formatter(value) if formatter else value
        executive_rows.append({"Indicator": label, "Value": formatted})

    def _format_peak_with_timestamp(
        value: object, timestamp: object
    ) -> str | int | None:
        if value is None:
            return None
        if isinstance(value, float) and pd.isna(value):
            return None
        try:
            numeric = int(value)
        except (TypeError, ValueError):
            return None
        if timestamp is None:
            return numeric
        formatted_ts = _format_timestamp(timestamp)
        if formatted_ts == "N/A":
            return numeric
        return f"{numeric} at {formatted_ts}"

    def _format_top_performer(info: object) -> str | None:
        if not isinstance(info, dict):
            return None
        antenna = info.get("antenna")
        if antenna is None:
            return None
        performer_label = str(antenna)
        participation = info.get("participation_pct")
        if participation is not None and not pd.isna(participation):
            performer_label += f" ({float(participation):.1f}% of reads)"
        reads_value = info.get("total_reads")
        if reads_value is not None and not pd.isna(reads_value):
            performer_label += f", {int(reads_value)} reads"
        return performer_label

    total_epcs: int | None = None
    if "EPC" in summary_epc.columns:
        try:
            total_epcs = int(summary_epc["EPC"].nunique())
        except Exception:
            total_epcs = None
    elif not summary_epc.empty:
        total_epcs = int(summary_epc.shape[0])
    if total_epcs is not None:
        _append_executive("Distinct EPCs", total_epcs, lambda value: int(value))

    if "total_reads" in summary_epc.columns:
        try:
            total_reads_val = int(summary_epc["total_reads"].sum())
            _append_executive("Total reads", total_reads_val, lambda value: int(value))
        except Exception:
            pass

    _append_executive(
        "Average dwell time (s)",
        metrics_info.get("average_dwell_seconds"),
        lambda value: round(float(value), 2),
    )
    _append_executive(
        "Maximum dwell time (s)",
        metrics_info.get("tag_dwell_time_max"),
        lambda value: round(float(value), 2),
    )
    _append_executive(
        "Throughput (distinct EPCs/min)",
        metrics_info.get("throughput_per_minute"),
        lambda value: round(float(value), 2),
    )
    _append_executive(
        "Session throughput (reads/min)",
        metrics_info.get("session_throughput"),
        lambda value: round(float(value), 2),
    )
    _append_executive(
        "Read continuity rate (%)",
        metrics_info.get("read_continuity_rate"),
        lambda value: round(float(value), 2),
    )
    _append_executive(
        "Session duration (s)",
        metrics_info.get("session_duration_seconds"),
        lambda value: round(float(value), 2),
    )
    _append_executive(
        "Active time with EPCs (s)",
        metrics_info.get("session_active_seconds"),
        lambda value: round(float(value), 2),
    )
    _append_executive(
        "Average concurrent EPCs",
        metrics_info.get("concurrency_average"),
        lambda value: round(float(value), 2),
    )
    _append_executive(
        "Congestion index (reads/s)",
        metrics_info.get("congestion_index"),
        lambda value: round(float(value), 2),
    )
    peak_concurrency_exec = metrics_info.get("concurrency_peak")
    peak_concurrency_repr = _format_peak_with_timestamp(
        peak_concurrency_exec, metrics_info.get("concurrency_peak_time")
    )
    if peak_concurrency_repr is not None:
        executive_rows.append(
            {"Indicator": "Peak concurrent EPCs", "Value": peak_concurrency_repr}
        )

    _append_executive(
        "Inactive periods (>5Ã window)",
        metrics_info.get("inactive_periods_count"),
        lambda value: int(value),
    )
    _append_executive(
        "Total inactive time (s)",
        metrics_info.get("inactive_total_seconds"),
        lambda value: round(float(value), 2),
    )
    _append_executive(
        "Longest inactive period (s)",
        metrics_info.get("inactive_longest_seconds"),
        lambda value: round(float(value), 2),
    )

    throughput_mean = metrics_info.get("epcs_per_minute_mean")
    _append_executive(
        "Average active EPCs/min",
        throughput_mean,
        lambda value: round(float(value), 2),
    )

    peak_epm_repr = _format_peak_with_timestamp(
        metrics_info.get("epcs_per_minute_peak"),
        metrics_info.get("epcs_per_minute_peak_time"),
    )
    if peak_epm_repr is not None:
        executive_rows.append(
            {"Indicator": "Peak active EPCs/min", "Value": peak_epm_repr}
        )

    dominant_exec = metrics_info.get("dominant_antenna")
    if dominant_exec is not None and str(dominant_exec) != "" and not (
        isinstance(dominant_exec, float) and pd.isna(dominant_exec)
    ):
        try:
            dominant_display = int(dominant_exec)
        except (TypeError, ValueError):
            dominant_display = dominant_exec
        executive_rows.append({"Indicator": "Dominant antenna", "Value": dominant_display})

    _append_executive(
        "Global RSSI mean (dBm)",
        metrics_info.get("global_rssi_avg"),
        lambda value: round(float(value), 2),
    )
    _append_executive(
        "Global RSSI std (dBm)",
        metrics_info.get("global_rssi_std"),
        lambda value: round(float(value), 2),
    )

    coverage_rate = structured_info.get("coverage_rate")
    if coverage_rate is not None and not pd.isna(coverage_rate):
        expected_total = structured_info.get("expected_total")
        expected_found = structured_info.get("expected_found")
        coverage_label = f"{float(coverage_rate):.2f}%"
        if expected_total is not None and expected_found is not None:
            coverage_label += f" ({int(expected_found)}/{int(expected_total)})"
        executive_rows.append({"Indicator": "Coverage rate", "Value": coverage_label})

    redundancy_exec = structured_info.get("tag_read_redundancy")
    if redundancy_exec is not None and not pd.isna(redundancy_exec):
        executive_rows.append(
            {
                "Indicator": "Tag read redundancy",
                "Value": f"{float(redundancy_exec):.2f}Ã",
            }
        )

    balance_exec = structured_info.get("antenna_balance")
    if balance_exec is not None and not pd.isna(balance_exec):
        executive_rows.append(
            {
                "Indicator": "Antenna balance (Ï)",
                "Value": f"{float(balance_exec):.2f}%",
            }
        )

    rssi_stability_exec = structured_info.get("rssi_stability_index")
    if rssi_stability_exec is not None and not pd.isna(rssi_stability_exec):
        executive_rows.append(
            {
                "Indicator": "RSSI stability index (Ï)",
                "Value": f"{float(rssi_stability_exec):.2f} dBm",
            }
        )

    top_performer_label = _format_top_performer(
        structured_info.get("top_performer_antenna")
    )
    if top_performer_label:
        executive_rows.append(
            {"Indicator": "Top performer antenna", "Value": top_performer_label}
        )

    structured_rows: list[dict[str, object]] = []
    missing_expected_df = pd.DataFrame()
    face_coverage_df = None
    row_coverage_df = None
    missing_positions_df = None
    hotspots_df: pd.DataFrame | None = None
    frequency_usage_df: pd.DataFrame | None = None
    location_errors_df: pd.DataFrame | None = None
    reads_by_face_df: pd.DataFrame | None = None
    if structured_info:
        coverage = structured_info.get("coverage_rate")
        expected_total = structured_info.get("expected_total")
        expected_found = structured_info.get("expected_found")
        if coverage is not None and not pd.isna(coverage):
            coverage_value = f"{float(coverage):.2f}%"
            if expected_total is not None and expected_found is not None:
                coverage_value += f" ({int(expected_found)}/{int(expected_total)})"
            structured_rows.append({"Metric": "Coverage rate", "Value": coverage_value})
        elif expected_total:
            structured_rows.append(
                {
                    "Metric": "Coverage rate",
                    "Value": f"0.00% (0/{int(expected_total)})",
                }
            )
        else:
            structured_rows.append({"Metric": "Coverage rate", "Value": "N/A"})

        redundancy = structured_info.get("tag_read_redundancy")
        if redundancy is not None and not pd.isna(redundancy):
            structured_rows.append(
                {
                    "Metric": "Tag read redundancy",
                    "Value": f"{float(redundancy):.2f}Ã",
                }
            )

        balance = structured_info.get("antenna_balance")
        if balance is not None and not pd.isna(balance):
            structured_rows.append(
                {
                    "Metric": "Antenna balance (Ï)",
                    "Value": f"{float(balance):.2f}%",
                }
            )

        rssi_stability = structured_info.get("rssi_stability_index")
        if rssi_stability is not None and not pd.isna(rssi_stability):
            structured_rows.append(
                {
                    "Metric": "RSSI stability index (Ï)",
                    "Value": f"{float(rssi_stability):.2f} dBm",
                }
            )

        top_performer = structured_info.get("top_performer_antenna")
        performer_label = _format_top_performer(top_performer)
        if performer_label:
            structured_rows.append(
                {
                    "Metric": "Top performer antenna",
                    "Value": performer_label,
                }
            )

        hotspot_count = structured_info.get("read_hotspots_count")
        threshold_value = structured_info.get("read_hotspots_threshold")
        if hotspot_count is not None and not pd.isna(hotspot_count):
            if hotspot_count and threshold_value is not None and not pd.isna(threshold_value):
                value = f"{int(hotspot_count)} (â¥ {float(threshold_value):.2f} reads)"
            elif hotspot_count:
                value = str(int(hotspot_count))
            else:
                value = "0"
            structured_rows.append({"Metric": "Read hotspots", "Value": value})

        frequency_unique = structured_info.get("frequency_unique_count")
        if frequency_unique is not None and not pd.isna(frequency_unique):
            structured_rows.append(
                {
                    "Metric": "Frequencies used",
                    "Value": int(frequency_unique),
                }
            )

        location_error_count = structured_info.get("location_error_count")
        if location_error_count is not None and not pd.isna(location_error_count):
            structured_rows.append(
                {
                    "Metric": "Location errors",
                    "Value": int(location_error_count),
                }
            )

        missing_full = structured_info.get("missing_expected_full") or []
        missing_suffix = structured_info.get("missing_expected_suffix") or []
        missing_expected_records = [
            {"Type": "Full EPC", "Identifier": token} for token in missing_full
        ]
        missing_expected_records.extend(
            {"Type": "Suffix", "Identifier": token} for token in missing_suffix
        )
        if missing_expected_records:
            missing_expected_df = pd.DataFrame(missing_expected_records)
        structured_rows.append(
            {
                "Metric": "Missing expected tags",
                "Value": len(missing_expected_records),
            }
        )

        layout_total = structured_info.get("layout_total_positions")
        layout_read = structured_info.get("layout_read_positions")
        if layout_total is not None and layout_read is not None:
            structured_rows.append(
                {
                    "Metric": "Layout positions covered",
                    "Value": f"{int(layout_read)}/{int(layout_total)}",
                }
            )

        face_coverage_df = structured_info.get("layout_face_coverage")
        if isinstance(face_coverage_df, pd.DataFrame) and not face_coverage_df.empty:
            face_coverage_df = face_coverage_df.copy()

        row_coverage_df = structured_info.get("layout_row_coverage")
        if isinstance(row_coverage_df, pd.DataFrame) and not row_coverage_df.empty:
            row_coverage_df = row_coverage_df.copy()
        else:
            row_coverage_df = None

        missing_positions_df = structured_info.get("missing_positions_table")
        if isinstance(missing_positions_df, pd.DataFrame) and not missing_positions_df.empty:
            missing_positions_df = missing_positions_df.copy()
        else:
            missing_positions_df = None

        hotspots_df = structured_info.get("read_hotspots")
        if isinstance(hotspots_df, pd.DataFrame) and not hotspots_df.empty:
            hotspots_df = hotspots_df.copy()
            if "z_score" in hotspots_df.columns:
                hotspots_df["z_score"] = hotspots_df["z_score"].astype(float).round(3)
        else:
            hotspots_df = None

        frequency_usage_df = structured_info.get("frequency_usage")
        if isinstance(frequency_usage_df, pd.DataFrame) and not frequency_usage_df.empty:
            frequency_usage_df = frequency_usage_df.copy()
            if "participation_pct" in frequency_usage_df.columns:
                frequency_usage_df["participation_pct"] = (
                    frequency_usage_df["participation_pct"].astype(float).round(2)
                )
        else:
            frequency_usage_df = None

        location_errors_df = structured_info.get("location_errors")
        if isinstance(location_errors_df, pd.DataFrame) and not location_errors_df.empty:
            location_errors_df = location_errors_df.copy()
        else:
            location_errors_df = None

        reads_by_face_df = structured_info.get("reads_by_face")
        if isinstance(reads_by_face_df, pd.DataFrame) and not reads_by_face_df.empty:
            reads_by_face_df = reads_by_face_df.copy()
            if "participation_pct" in reads_by_face_df.columns:
                reads_by_face_df["participation_pct"] = (
                    reads_by_face_df["participation_pct"].astype(float).round(2)
                )
        else:
            reads_by_face_df = None

    with pd.ExcelWriter(out, engine="xlsxwriter") as writer:
        summary_epc.to_excel(writer, index=False, sheet_name=SHEET_RESUMO)
        unexpected_df = unexpected
        if unexpected_df is None:
            unexpected_df = pd.DataFrame(columns=summary_epc.columns)
        unexpected_df.to_excel(writer, index=False, sheet_name=SHEET_UNEXPECTED)
        ant_counts.to_excel(writer, index=False, sheet_name=SHEET_ANTENNA)
        if positions_df is not None:
            sheet_posicoes = SHEET_POSICOES
            positions_to_write = positions_df.copy()
            positions_to_write.to_excel(writer, index=False, sheet_name=sheet_posicoes)
            next_row = len(positions_to_write) + 2
            if isinstance(reads_by_face_df, pd.DataFrame) and not reads_by_face_df.empty:
                reads_face_to_write = reads_by_face_df.rename(
                    columns={
                        "total_positions": "Total positions",
                        "positions_with_reads": "Positions with reads",
                        "total_reads": "Total reads",
                        "participation_pct": "Participation (%)",
                    }
                )
                reads_face_to_write.to_excel(
                    writer,
                    index=False,
                    sheet_name=sheet_posicoes,
                    startrow=next_row,
                )
                next_row += len(reads_face_to_write) + 2
        if metadata:
            md_df = pd.DataFrame(list(metadata.items()), columns=["Key", "Value"])
            md_df.to_excel(writer, index=False, sheet_name=SHEET_METADATA)

        exec_sheet = SHEET_EXECUTIVE
        if executive_rows:
            exec_df = pd.DataFrame(executive_rows)
        else:
            exec_df = pd.DataFrame(
                [
                    {
                        "Indicator": "Message",
                        "Value": "No executive indicators available.",
                    }
                ]
            )
        exec_df.to_excel(writer, index=False, sheet_name=exec_sheet)

        if structured_info:
            sheet_structured = SHEET_STRUCTURED
            structured_row = 0
            if structured_rows:
                structured_df = pd.DataFrame(structured_rows)
                structured_df.to_excel(
                    writer,
                    index=False,
                    sheet_name=sheet_structured,
                    startrow=structured_row,
                )
                structured_row += len(structured_df) + 2
            if not missing_expected_df.empty:
                missing_expected_df.to_excel(
                    writer,
                    index=False,
                    sheet_name=sheet_structured,
                    startrow=structured_row,
                )
                structured_row += len(missing_expected_df) + 2
            if isinstance(face_coverage_df, pd.DataFrame) and not face_coverage_df.empty:
                face_to_write = face_coverage_df.rename(
                    columns={
                        "total_positions": "Total positions",
                        "read_positions": "Read positions",
                        "coverage_pct": "Coverage (%)",
                        "total_reads": "Total reads",
                    }
                )
                face_to_write.to_excel(
                    writer,
                    index=False,
                    sheet_name=sheet_structured,
                    startrow=structured_row,
                )
                structured_row += len(face_to_write) + 2
            if isinstance(row_coverage_df, pd.DataFrame) and not row_coverage_df.empty:
                row_to_write = row_coverage_df.rename(
                    columns={
                        "total_positions": "Total positions",
                        "read_positions": "Read positions",
                        "coverage_pct": "Coverage (%)",
                        "total_reads": "Total reads",
                    }
                )
                row_to_write.to_excel(
                    writer,
                    index=False,
                    sheet_name=sheet_structured,
                    startrow=structured_row,
                )
                structured_row += len(row_to_write) + 2
            if isinstance(reads_by_face_df, pd.DataFrame) and not reads_by_face_df.empty:
                reads_face_to_write = reads_by_face_df.rename(
                    columns={
                        "total_positions": "Total positions",
                        "positions_with_reads": "Positions with reads",
                        "total_reads": "Total reads",
                        "participation_pct": "Participation (%)",
                    }
                )
                reads_face_to_write.to_excel(
                    writer,
                    index=False,
                    sheet_name=sheet_structured,
                    startrow=structured_row,
                )
                structured_row += len(reads_face_to_write) + 2
            if isinstance(hotspots_df, pd.DataFrame) and not hotspots_df.empty:
                hotspots_to_write = hotspots_df.rename(
                    columns={
                        "EPC_suffix3": "Suffix",
                        "total_reads": "Total reads",
                        "expected_epc": "Expected?",
                        "pallet_position": "Pallet position",
                        "z_score": "Z-score",
                    }
                )
                hotspots_to_write.to_excel(
                    writer,
                    index=False,
                    sheet_name=sheet_structured,
                    startrow=structured_row,
                )
                structured_row += len(hotspots_to_write) + 2
            if isinstance(location_errors_df, pd.DataFrame) and not location_errors_df.empty:
                location_to_write = location_errors_df.rename(
                    columns={
                        "EPC_suffix3": "Suffix",
                        "total_reads": "Total reads",
                        "ExpectedEPC": "Expected EPC",
                        "ExpectedPosition": "Expected position",
                        "ObservedPosition": "Observed position",
                    }
                )
                location_to_write.to_excel(
                    writer,
                    index=False,
                    sheet_name=sheet_structured,
                    startrow=structured_row,
                )
                structured_row += len(location_to_write) + 2
            if isinstance(frequency_usage_df, pd.DataFrame) and not frequency_usage_df.empty:
                frequency_to_write = frequency_usage_df.rename(
                    columns={
                        "frequency_mhz": "Frequency (MHz)",
                        "read_count": "Read count",
                        "participation_pct": "Participation (%)",
                    }
                )
                frequency_to_write.to_excel(
                    writer,
                    index=False,
                    sheet_name=sheet_structured,
                    startrow=structured_row,
                )
                structured_row += len(frequency_to_write) + 2
            if isinstance(missing_positions_df, pd.DataFrame) and not missing_positions_df.empty:
                missing_positions_df.to_excel(
                    writer,
                    index=False,
                    sheet_name=sheet_structured,
                    startrow=structured_row,
                )
                structured_row += len(missing_positions_df) + 2
            if structured_row == 0:
                pd.DataFrame({"Message": ["No structured metrics available."]}).to_excel(
                    writer,
                    index=False,
                    sheet_name=sheet_structured,
                )

        metrics_rows: list[dict[str, object]] = []
        _append_metric(
            "Average dwell time (s)",
            metrics_info.get("average_dwell_seconds"),
            lambda value: round(float(value), 2),
        )
        _append_metric(
            "Maximum dwell time (s)",
            metrics_info.get("tag_dwell_time_max"),
            lambda value: round(float(value), 2),
        )
        _append_metric(
            "Entry/exit events",
            metrics_info.get("total_events"),
            lambda value: int(value),
        )
        dominant = metrics_info.get("dominant_antenna")
        if dominant is not None and str(dominant) != "" and not (
            isinstance(dominant, float) and pd.isna(dominant)
        ):
            try:
                display_dominant = int(dominant)
            except (TypeError, ValueError):
                display_dominant = dominant
            metrics_rows.append(
                {
                    "Metric": "Dominant antenna",
                    "Value": display_dominant,
                }
            )
        elif metrics_info:
            metrics_rows.append(
                {
                    "Metric": "Dominant antenna",
                    "Value": "N/A",
                }
            )
        _append_metric(
            "Average active EPCs/min",
            metrics_info.get("epcs_per_minute_mean"),
            lambda value: round(float(value), 2),
        )
        if peak_epm_repr is not None:
            metrics_rows.append(
                {
                    "Metric": "Peak active EPCs/min",
                    "Value": peak_epm_repr,
                }
            )
        elif metrics_info:
            metrics_rows.append(
                {
                    "Metric": "Peak active EPCs/min",
                    "Value": "N/A",
                }
            )
        _append_metric(
            "Throughput (distinct EPCs/min)",
            metrics_info.get("throughput_per_minute"),
            lambda value: round(float(value), 2),
        )
        _append_metric(
            "Session throughput (reads/min)",
            metrics_info.get("session_throughput"),
            lambda value: round(float(value), 2),
        )
        _append_metric(
            "Read continuity rate (%)",
            metrics_info.get("read_continuity_rate"),
            lambda value: round(float(value), 2),
        )
        _append_metric(
            "Session start",
            metrics_info.get("session_start"),
            _format_timestamp,
        )
        _append_metric(
            "Session end (last read)",
            metrics_info.get("session_end"),
            _format_timestamp,
        )
        _append_metric(
            "Session end (with window)",
            metrics_info.get("session_end_with_grace"),
            _format_timestamp,
        )
        _append_metric(
            "Session duration (s)",
            metrics_info.get("session_duration_seconds"),
            lambda value: round(float(value), 2),
        )
        _append_metric(
            "Active time with EPCs (s)",
            metrics_info.get("session_active_seconds"),
            lambda value: round(float(value), 2),
        )
        _append_metric(
            "Average concurrent EPCs",
            metrics_info.get("concurrency_average"),
            lambda value: round(float(value), 2),
        )
        _append_metric(
            "Congestion index (reads/s)",
            metrics_info.get("congestion_index"),
            lambda value: round(float(value), 2),
        )
        peak_concurrency_repr = _format_peak_with_timestamp(
            metrics_info.get("concurrency_peak"),
            metrics_info.get("concurrency_peak_time"),
        )
        if peak_concurrency_repr is not None:
            metrics_rows.append(
                {
                    "Metric": "Peak concurrent EPCs",
                    "Value": peak_concurrency_repr,
                }
            )
        elif metrics_info:
            metrics_rows.append(
                {
                    "Metric": "Peak concurrent EPCs",
                    "Value": "N/A",
                }
            )

        _append_metric(
            "Inactive periods (>5Ã window)",
            metrics_info.get("inactive_periods_count"),
            lambda value: int(value),
        )
        _append_metric(
            "Total inactive time (s)",
            metrics_info.get("inactive_total_seconds"),
            lambda value: round(float(value), 2),
        )
        _append_metric(
            "Longest inactive period (s)",
            metrics_info.get("inactive_longest_seconds"),
            lambda value: round(float(value), 2),
        )
        _append_metric(
            "Global RSSI mean (dBm)",
            metrics_info.get("global_rssi_avg"),
            lambda value: round(float(value), 2),
        )
        _append_metric(
            "Global RSSI std (dBm)",
            metrics_info.get("global_rssi_std"),
            lambda value: round(float(value), 2),
        )

        alerts_lines = _build_alert_lines()

        if (
            metrics_rows
            or alerts_lines
            or timeline_df is not None
            or per_minute_df is not None
            or inactive_df is not None
        ):
            sheet_name = SHEET_FLUXO
            start_row = 0
            if metrics_rows:
                metrics_df = pd.DataFrame(metrics_rows)
                metrics_df.to_excel(
                    writer,
                    index=False,
                    sheet_name=sheet_name,
                    startrow=start_row,
                )
                start_row += len(metrics_rows) + 2
            if alerts_lines:
                alerts_df = pd.DataFrame({"Alerts": alerts_lines})
                alerts_df.to_excel(
                    writer,
                    index=False,
                    sheet_name=sheet_name,
                    startrow=start_row,
                )
                start_row += len(alerts_lines) + 2
            elif start_row and metrics_rows:
                start_row += 1
            if per_minute_df is not None and not per_minute_df.empty:
                per_minute_to_write = per_minute_df.copy()
                if "minute" in per_minute_to_write.columns:
                    per_minute_to_write["minute"] = per_minute_to_write["minute"].dt.strftime(
                        "%Y-%m-%d %H:%M"
                    )
                per_minute_to_write.to_excel(
                    writer,
                    index=False,
                    sheet_name=sheet_name,
                    startrow=start_row,
                )
                start_row += len(per_minute_to_write) + 2
            if inactive_df is not None and not inactive_df.empty:
                inactive_to_write = inactive_df.copy()
                for column in ("start_time", "end_time"):
                    if column in inactive_to_write.columns:
                        inactive_to_write[column] = inactive_to_write[column].dt.strftime(
                            "%Y-%m-%d %H:%M:%S"
                        )
                for numeric_column in ("duration_seconds", "gap_seconds", "gap_multiplier"):
                    if numeric_column in inactive_to_write.columns:
                        inactive_to_write[numeric_column] = inactive_to_write[
                            numeric_column
                        ].round(2)
                inactive_to_write.to_excel(
                    writer,
                    index=False,
                    sheet_name=sheet_name,
                    startrow=start_row,
                )
                start_row += len(inactive_to_write) + 2
            if concurrency_df is not None and not concurrency_df.empty:
                concurrency_to_write = concurrency_df.copy()
                if "timestamp" in concurrency_to_write.columns:
                    concurrency_to_write["timestamp"] = concurrency_to_write["timestamp"].dt.strftime(
                        "%Y-%m-%d %H:%M:%S"
                    )
                concurrency_to_write.to_excel(
                    writer,
                    index=False,
                    sheet_name=sheet_name,
                    startrow=start_row,
                )
                start_row += len(concurrency_to_write) + 2
            if timeline_df is not None:
                timeline_to_write = timeline_df.copy()
                timeline_to_write.to_excel(
                    writer,
                    index=False,
                    sheet_name=sheet_name,
                    startrow=start_row,
                )
            elif start_row == 0:
                pd.DataFrame(
                    {"Message": ["No continuous flow data available."]}
                ).to_excel(
                    writer,
                    index=False,
                    sheet_name=sheet_name,
                )
